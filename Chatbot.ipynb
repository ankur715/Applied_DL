{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Chatbot : PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  #neural networks\n",
    "from torch import optim  #optimizer\n",
    "import torch.nn.functional as F  #relu, softmax, etc.\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True if GPU\n",
    "CUDA = torch.cuda.is_available()\n",
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfer data in GPU, or CPU if not GPU\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_filepath = os.path.join(\"cornell movie-dialogs corpus\",\"movie_lines.txt\")\n",
    "conv_filepath = os.path.join(\"cornell movie-dialogs corpus\",\"movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
      "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
      "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
      "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
      "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n"
     ]
    }
   ],
   "source": [
    "# visualize some lines\n",
    "with open(lines_filepath, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "for line in lines[:8]:\n",
    "    print(line.strip())\n",
    "    \n",
    "# fields: lineID, charID, movieID, charname, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each line into dictionary of fields\n",
    "line_fields = [\"lineID\", \"charID\", \"movieID\", \"charname\", \"line\"]\n",
    "lines = {}\n",
    "with open(lines_filepath, \"r\", encoding=\"iso-8859-1\") as f:  \n",
    "    \"\"\"\n",
    "    UTF-8 is a multibyte encoding that can represent any Unicode character. \n",
    "    ISO 8859-1 is a single-byte encoding that can represent the first 256 Unicode characters. \n",
    "    Both encode ASCII exactly the same way.    \n",
    "    \"\"\"\n",
    "    for line in f:\n",
    "        values = line.split(\" +++$+++ \")\n",
    "        # extract fields\n",
    "        lineObj = {}\n",
    "        for i,field in enumerate(line_fields):\n",
    "            lineObj[field] = values[i]\n",
    "        lines[lineObj[\"lineID\"]] = lineObj  #each line corresponding to lineID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       " '',\n",
       " \"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\"]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group fields from 'loadlines' into conversation based on movie_conversations.txt\n",
    "conv_fields = [\"char1ID\", \"char1ID\", \"movieID\", \"utteranceIDs\"]\n",
    "conversations = []\n",
    "with open(conv_filepath, \"r\", encoding=\"iso-8859-1\") as f:  \n",
    "    for line in f:\n",
    "        values = line.split(\" +++$+++ \")\n",
    "        # extract fields\n",
    "        convObj = {}\n",
    "        for i,field in enumerate(conv_fields):\n",
    "            convObj[field] = values[i]\n",
    "        # convert string results from split to list\n",
    "        lineIds = eval(convObj[\"utteranceIDs\"])\n",
    "        # reassemble lines\n",
    "        convObj[\"lines\"] = []\n",
    "        for lineId in lineIds:\n",
    "            convObj[\"lines\"].append(lines[lineId])            \n",
    "        conversations.append(convObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char1ID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n",
       " 'lines': [{'lineID': 'L194',\n",
       "   'charID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'charname': 'BIANCA',\n",
       "   'line': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'},\n",
       "  {'lineID': 'L195',\n",
       "   'charID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'charname': 'CAMERON',\n",
       "   'line': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\"},\n",
       "  {'lineID': 'L196',\n",
       "   'charID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'charname': 'BIANCA',\n",
       "   'line': 'Not the hacking and gagging and spitting part.  Please.\\n'},\n",
       "  {'lineID': 'L197',\n",
       "   'charID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'charname': 'CAMERON',\n",
       "   'line': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pairs of sentences from conversations\n",
    "qa_pairs = []\n",
    "for conversation in conversations:\n",
    "    # iterate over all lines of conversations\n",
    "    for i in range(len(conversation[\"lines\"]) - 1): \n",
    "        inputLine = conversation[\"lines\"][i][\"line\"].strip()\n",
    "        targetLine = conversation[\"lines\"][i+1][\"line\"].strip()\n",
    "        # filter wrong samples (if one of lists is empty)\n",
    "        if inputLine and targetLine:\n",
    "            qa_pairs.append([inputLine, targetLine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\"],\n",
       " [\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       "  'Not the hacking and gagging and spitting part.  Please.'],\n",
       " ['Not the hacking and gagging and spitting part.  Please.',\n",
       "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221282"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to new file\n",
    "datafile = os.path.join(\"cornell movie-dialogs corpus\", \"formatted_movie_lines.txt\")\n",
    "delimiter = \"\\t\" \n",
    "# unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing newly formatted file...\n",
      "Done writing to file\n"
     ]
    }
   ],
   "source": [
    "# write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, \"w\", encoding=\"utf-8\") as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter) \n",
    "    # seprarated qa pair by tab\n",
    "    for pair in qa_pairs:\n",
    "        writer.writerow(pair)\n",
    "        # separated conversations by new line \n",
    "print(\"Done writing to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\r\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\r\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\r\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "# visualize some lines\n",
    "datafile = os.path.join(\"cornell movie-dialogs corpus\", \"formatted_movie_lines.txt\")\n",
    "with open(datafile, \"rb\") as file:\n",
    "    lines = file.readlines()\n",
    "for line in lines[:8]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  #used for padding short sentences \n",
    "SOS_token = 1  #start-of-sentence token\n",
    "EOS_token = 2  #end-of-sentence token\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\", EOS_token:\"EOS\"}\n",
    "        self.num_words = 3  #count SOS, EOS, PAD\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    # remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        keep_words = []\n",
    "        for k,v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "                \n",
    "        print(\"keep_words {} / {} = {:.4f}\".format(len(keep_words), \n",
    "                                                   len(self.word2index),\n",
    "                                                   len(keep_words) / \n",
    "                                                  len(self.word2index)))\n",
    "        # reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\", EOS_token:\"EOS\"}\n",
    "        self.num_words = 3  #count SOS, EOS, PAD\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Montreal,Francoise....'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "unicodeToAscii(\"Montréal,Françoise....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase, trim white spaces, lines... etc, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # replace any .!? by a whitespace + the character -> '!' = ' !'. \\1 means the first bracketed group -> [,!?].\n",
    "    # r is to not consider \\1 as a character (r to escape a backslash). + means one or more\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # remove any character that is not a sequence of lower or upper case letters\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    # remove a sequence of whitespace characters\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa aa !s s dd ?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeString(\"aa123aa!s's  dd?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Text - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing file... Please wait\n",
      "Done Reading!\n"
     ]
    }
   ],
   "source": [
    "datafile = os.path.join(\"cornell movie-dialogs corpus\", \"formatted_movie_lines.txt\")\n",
    "# read the file ad split into lines\n",
    "print(\"Reading and processing file... Please wait\")\n",
    "lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "# split every line into pairs and normalize \n",
    "pairs = [[normalizeString(s) for s in pair.split(\"\\t\")] for pair in lines]\n",
    "print(\"Done Reading!\")\n",
    "voc = Vocabulary(\"cornell movie-dialogs corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       " \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0].split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
       "  'well i thought we d start with pronunciation if that s okay with you .'],\n",
       " [''],\n",
       " ['well i thought we d start with pronunciation if that s okay with you .',\n",
       "  'not the hacking and gagging and spitting part . please .']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442563"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true if both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "MAX_LENGTH = 10  #maximum sentence length (max words)\n",
    "def filterPair(p):\n",
    "    # input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter pair using filterPair(pair)\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 221282 pairs/conversations in the dataset\n",
      "After filtering, there are 64271 pairs/conversations\n"
     ]
    }
   ],
   "source": [
    "pairs = [pair for pair in pairs if len(pair)>1]\n",
    "print(\"There are {} pairs/conversations in the dataset\".format(len(pairs)))\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"After filtering, there are {} pairs/conversations\".format(len(pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Rid of Rare Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words: 42989\n",
      "['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .', 'well i thought we d start with pronunciation if that s okay with you .']\n",
      "['']\n",
      "['well i thought we d start with pronunciation if that s okay with you .', 'not the hacking and gagging and spitting part . please .']\n",
      "['']\n",
      "['not the hacking and gagging and spitting part . please .', 'okay . . . then how bout we try out some french cuisine . saturday ? night ?']\n",
      "['']\n",
      "['you re asking me out . that s so cute . what s your name again ?', 'forget it .']\n",
      "['']\n",
      "['no no it s my fault we didn t have a proper introduction', 'cameron .']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "#loop through each pair of and add the question and reply sentence to the vocabulary\n",
    "for pair in pairs:\n",
    "    voc.addSentence(pair[0])\n",
    "    voc.addSentence(pair[1])\n",
    "print(\"Counted words:\", voc.num_words)\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 3  #minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # trim words used under the MIN_COUNT from the voc\n",
    "#     voc.trim(MIN_COUNT)\n",
    "    # filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # check input sentence\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # check output sentence\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        \n",
    "        # only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(\" \")] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 2]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "indexesFromSentence(voc, pairs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .', '', 'well i thought we d start with pronunciation if that s okay with you .', '', 'not the hacking and gagging and spitting part . please .', '', 'you re asking me out . that s so cute . what s your name again ?', '', 'no no it s my fault we didn t have a proper introduction', '']\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  25,\n",
       "  2],\n",
       " [39, 2],\n",
       " [27, 28, 29, 4, 30, 31, 32, 33, 34, 35, 36, 37, 32, 38, 25, 2],\n",
       " [39, 2],\n",
       " [40, 23, 41, 11, 42, 11, 43, 44, 25, 45, 25, 2],\n",
       " [39, 2],\n",
       " [38, 46, 47, 48, 49, 25, 35, 36, 50, 51, 25, 52, 36, 53, 54, 26, 8, 2],\n",
       " [39, 2],\n",
       " [55, 55, 56, 36, 57, 58, 4, 59, 60, 61, 62, 63, 64, 2],\n",
       " [39, 2]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define some samples for testing\n",
    "inp = []\n",
    "out = []\n",
    "i = 0\n",
    "for pair in pairs[:10]:\n",
    "    inp.append(pair[0])\n",
    "    out.append(pair[1])\n",
    "print(inp)\n",
    "print(len(inp))\n",
    "indexes = [indexesFromSentence(voc, sentence) for sentence in inp]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Zip Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1), ('B', 2), ('C', 3)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"A\",\"B\",\"C\"]\n",
    "b = [1,2,3]\n",
    "list(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1), ('B', 2), ('C', 3), ('D', None), ('E', None)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "b = [1,2,3]\n",
    "list(itertools.zip_longest(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 39, 27, 40, 38, 55, 39),\n",
       " (4, 35, 28, 23, 46, 55, 2),\n",
       " (5, 2, 29, 41, 47, 56, 'x'),\n",
       " (2, 'x', 4, 11, 48, 36, 'x'),\n",
       " ('x', 'x', 30, 42, 49, 57, 'x'),\n",
       " ('x', 'x', 31, 11, 21, 58, 'x'),\n",
       " ('x', 'x', 35, 43, 25, 4, 'x'),\n",
       " ('x', 'x', 36, 44, 52, 59, 'x'),\n",
       " ('x', 'x', 37, 25, 36, 60, 'x'),\n",
       " ('x', 'x', 32, 45, 53, 61, 'x'),\n",
       " ('x', 'x', 38, 25, 54, 62, 'x'),\n",
       " ('x', 'x', 25, 2, 26, 63, 'x'),\n",
       " ('x', 'x', 2, 'x', 8, 64, 'x'),\n",
       " ('x', 'x', 'x', 'x', 2, 2, 'x')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[3, 4, 5, 2],\n",
    " [39, 35, 2],\n",
    " [27, 28, 29, 4, 30, 31, 35, 36, 37, 32, 38, 25, 2],\n",
    " [40, 23, 41, 11, 42, 11, 43, 44, 25, 45, 25, 2],\n",
    " [38, 46, 47, 48, 49, 21, 25, 52, 36, 53, 54, 26, 8, 2],\n",
    " [55, 55, 56, 36, 57, 58, 4, 59, 60, 61, 62, 63, 64, 2],\n",
    " [39, 2]]\n",
    "list(itertools.zip_longest(*a, fillvalue=\"x\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l, fillvalue=0):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng = [len(ind) for ind in indexes]\n",
    "max(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 2, 16, 2, 12, 2, 18, 2, 14, 2]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 39, 27, 39, 40, 39, 38, 39, 55, 39),\n",
       " (4, 2, 28, 2, 23, 2, 46, 2, 55, 2),\n",
       " (5, 0, 29, 0, 41, 0, 47, 0, 56, 0),\n",
       " (6, 0, 4, 0, 11, 0, 48, 0, 36, 0),\n",
       " (7, 0, 30, 0, 42, 0, 49, 0, 57, 0),\n",
       " (8, 0, 31, 0, 11, 0, 25, 0, 58, 0),\n",
       " (9, 0, 32, 0, 43, 0, 35, 0, 4, 0),\n",
       " (10, 0, 33, 0, 44, 0, 36, 0, 59, 0),\n",
       " (11, 0, 34, 0, 25, 0, 50, 0, 60, 0),\n",
       " (12, 0, 35, 0, 45, 0, 51, 0, 61, 0),\n",
       " (13, 0, 36, 0, 25, 0, 25, 0, 62, 0),\n",
       " (14, 0, 37, 0, 2, 0, 52, 0, 63, 0),\n",
       " (15, 0, 32, 0, 0, 0, 36, 0, 64, 0),\n",
       " (16, 0, 38, 0, 0, 0, 53, 0, 2, 0),\n",
       " (17, 0, 25, 0, 0, 0, 54, 0, 0, 0),\n",
       " (18, 0, 2, 0, 0, 0, 26, 0, 0, 0),\n",
       " (19, 0, 0, 0, 0, 0, 8, 0, 0, 0),\n",
       " (20, 0, 0, 0, 0, 0, 2, 0, 0, 0),\n",
       " (21, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (22, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (23, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (24, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (25, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (26, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (25, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " (2, 0, 0, 0, 0, 0, 0, 0, 0, 0)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "test_result = zeroPadding(indexes)\n",
    "print(len(test_result))  #max length is now the number of rows or the batch size\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data - Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMatrix(l, value=0):\n",
    "    m = []\n",
    "    for i,seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_result = binaryMatrix(test_result)\n",
    "binary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return padded input sequence tensor and as well as a tensor of lengths for each of the sequances in the batch\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data - Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    # sort the questions in descending length\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    # assert len(inp) == lengths[0]\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable:\n",
      " tensor([[34128,    39,    39,    39,    39],\n",
      "        [   25,     2,     2,     2,     2],\n",
      "        [   25,     0,     0,     0,     0],\n",
      "        [   25,     0,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]])\n",
      "lengths: tensor([5, 2, 2, 2, 2])\n",
      "target_variable:\n",
      " tensor([[34128,    39,    39,    39,    39],\n",
      "        [   25,     2,     2,     2,     2],\n",
      "        [   25,     0,     0,     0,     0],\n",
      "        [   25,     0,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]])\n",
      "mask:\n",
      " tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0]], dtype=torch.uint8)\n",
      "max_target_len: 5\n"
     ]
    }
   ],
   "source": [
    "# example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\\n\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\\n\", target_variable)\n",
    "print(\"mask:\\n\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
